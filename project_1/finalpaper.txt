Abstract
Atrial fibrillation is a cardiac dysrhythmia where the atria do not contract properly. It affects more people than any other dysrhythmia, and accurate detection of it is incredibly important for their well being. This algorithm looks at entropy, or uncertainty, in normal sinus rhythms versus signals with atrial fibrillation as well as signals with other dysrhythmias. Analysis found that eliminating the ventricular activity through means of either wavelets or cancellation of the QRS-T segment and then finding the entropy of only the atrial activity was not conclusive enough for classification. Instead, classifying normal versus atrial fibrillation was successful 100 percent of the time by calculating the entropy of the Q to Q interval. Heart rate variability was much higher in signals with atrial fibrillation than normal signals. If further research could be done to improve atrial activity isolation, entropy of the wavelet determined atrial activity could be informative enough to make a classification. This could make an algorithm that is much more robust and one that is able to definitively detect atrial fibrillation in lieu of any other dysrhythmia.
Introduction
Atrial fibrillation (AF) is the most common cardiac dysrhythmia. It is characterized by uncoordinated atrial activation, leading to inefficient pumping of blood into the ventricles and out into the rest of the body [1]. Since this condition is so common, the ability to accurately detect it in an electrocardiogram (ECG) signal is imperative. This problem has been solved in many different ways, but each is subject to certain issues that arise, making them less reliable. One common method is to detect the R to R time of the ventricular activity [1]. As atrial activity in a patient with AF is erratic, the R to R interval is less consistent than in a patient with regular atrial activity. However, this is also the case in many other dysrhythmias, so that single factor is not enough to definitively diagnose AF [2]. Therefore, with this method, it must be known that AF is a possibility and there must be other ways to ensure that AF is the true cause of the R to R invariability. Another common method is to cancel out the ventricular activity in the ECG and then analyze the behavior of only the atrial activity [3]. In a signal that has AF, p-waves may come multiple at a time or be skipped between ventricular contractions. This is a reliable factor to investigate for the diagnosis of AF. This method, however, can be susceptible to inconsistencies in results due to noise. Additionally, detection based on p-wave activity may be inconsistent because of the episodic nature of AF [4]. Since irregular atrial activity may come and go, it may not be consistent enough for an algorithm that looks only for p-wave activity as enough of the signal may have normal activity to result in a normal classification.
In an attempt to mitigate the issues that other methods face, this project focused on the comparison of classification for time and frequency domain methods. In the time domain method, the QRS-T portion of the ECG, or the ventricular activity, was detected. All of these portions from each signal were averaged to create a sample of the average QRS-T portion. Subtracting this average from each pulse allowed for most of the ventricular activity to be cancelled out, leaving only the atrial activity for further analysis. The frequency domain method involved wavelets. The ECG signal was broken down into ten different frequency bands from zero up to half of the sample frequency of the data, 150 Hz. The frequency bands that make up the atrial activity were summed in the time domain and the frequency bands that make up the ventricular activity were left out. The results of this step had the same intent as cancelling out the QRS-T portion from the time domain. In both methods, once atrial activity was the only thing remaining in the signal, entropy was used to determine the amount of uncertainty that existed. Ideally, in a normal sinus rhythm, the uncertainty and, therefore, the entropy, should be low due to the consistent nature of the signal. However, in a signal that displays AF, atrial activity is more  inconsistent, so uncertainty and entropy should high. This would allow for simple classification of the signals. Additionally, entropy calculations were performed on Q to Q intervals in each signal as another metric to investigate.
This analysis was performed on single-lead ECG data that was obtained from over 12,000 people. Data in this set included normal sinus rhythm, atrial fibrillation data, noisy data, and data with other dysrhythmias [5]. For the purpose of this project, only low noise, normal sinus rhythm and atrial fibrillation data was considered along with signals that exhibited other dysrhythmias for comparative analysis. 
Methods
The data used for this project was obtained from the Physionet Computing in Cardiology Challenge 2017 dataset. This dataset contained 12,186 ECG recordings that were donated by AliveCor. The recordings were taken by individuals who had purchased an AliveCor single-channel ECG recording device. These individuals held an electrode in each hand to create a lead I equivalent circuit. The device checked for signal quality and then recorded data for 30 seconds. This data was sent acoustically to a smartphone with a 19.1 kHz carrier frequency and a 200 Hz/mV modulation index. This data was stored with a 300 Hz sampling frequency in 16 bit files [2].
The dataset included labels for each signal to denote normal sinus rhythm, atrial fibrillation, other dysrhythmias, and signals that were too noisy to be classified. These labels were used as benchmarks to test algorithm accuracy against. Ten normal sinus rhythms, ten atrial fibrillation rhythms, and ten other dysrhythmia signals were selected as the training set for this project and ten more of each were selected as the test set.
This algorithm was split into three methods. For the first two methods, the goal was to remove all ventricular activity, leaving only atrial activity to be analyzed. This was split into two methods to determine if either the time or frequency domain was the best to complete the ventricular removal. The third method looked at classification without the removal of the ventricular activity.
The first method that was used to complete AF classification involved the frequency domain. For this analysis, no preliminary data manipulation such as filtering was implemented aside from normalizing the ECG data so that it was between zero and one. Since frequency analysis was being used, it was assumed that any noise would be filtered out through removal of the high frequency content. The original ECG signal was sent through wavelet decomposition. The discrete approximation of the Meyer wavelet was used as the mother wavelet for this technique. Nine different detail coefficients and one approximate coefficient was found. These coefficients led to ten different frequency bands from zero to half of the sampling frequency. As it is known that the QRS complex lies from about 8 to 50 Hz [7], the bands that included these frequencies were eliminated from further analysis. Beyond that, a trial and error method was used to determine which of the low frequency coefficients were necessary to capture atrial activity and eliminate ventricular activity. It was found that the approximate coefficient, a9, and the first two detail coefficients, d9 and d8, were the coefficients that encompassed only the atrial activity. The values of these coefficients were added together to create a new signal that would, theoretically, display only the desired content.
Once the atrial activity signal was obtained, wavelet entropy and entropy were both calculated for it. For wavelet entropy, the three coefficients that were used to create the atrial activity signal were put through equations 1 and 2 in order to determine the resulting amount of uncertainty in the signal [4].
An additional form of entropy was calculated that did not take any of the wavelet coefficients into account. Instead, it only looked at the probabilities of the atrial activity signal. This was done for windows of one second of data in order to reduce the effects of significant spikes in the data. The entropy value of every second was averaged to obtain one final result. The probabilities for values in each second of data were put through equation 3 to determine the result.
Next, the second method was completed. This method removed the ventricular activity using the signal in the time domain. In order to account for some of the noise, the first thing that was done in this method was bandpass filter the data. The pass band went from 5 to 15 Hz. This allowed for baseline wander and other low frequency noise along with high frequency noise to be removed from the signal. Then, to account for any signals that contained outliers, the peaks of the signal were found. A threshold of 0.6 was set to allow a decision to be made on whether the peaks were a QRS complex or a smaller T or P wave peak. Next, any of those peaks that were greater than 1.5 times the median value of the QRS peaks were determined to be outliers, or noisy data. That peak was then removed from the data by setting all of the points involved with it to the average value of the signal. This allowed for the peaks to be a consistent amplitude for further analysis.
Next, the Pan-Tompkins method was applied [6]. Here, the derivative of the ECG signal was taken. Then, the derivative of that result was taken again. The peaks in that second derivative of the ECG signal were found. A threshold of 0.3 times the maximum peak value was set in order to determine which of the peaks in the Pan-Tompkins derivative were significant. Those peaks were in the same location as the Q and S points on the ECG for each pulse, respectively. A test was done to determine if the first peak and, therefore each subsequent peak, were classified properly as a Q or S point. By default, the first peak was labelled a Q peak, however, if the next peak was more than 100 data points away, it was clear that it was not close enough to be an S point on the other end of the same QRS complex. Therefore, it was shown that the first peak must have been an S point and the classification of each peak was switched. Finally, before the Q and S points could be solidified, the algorithm went through each peak to determine if the set threshold had missed any points. It did this by checking if each Q and S point of the same index were within 100 points of one another. If they were not, it showed that one of them had been missed. Therefore, a new S point was added into the S points array at a location equal to the average Q to S interval. Then, the classification of each of the following points as Q or S was switched to account for the added point. Since T-wave location will be determined based on the location of S points, this step was very important for the integrity of locating the ventricular activity.
After the Q and S points were detected, the data was smoothed twice using a moving average filter. This was necessary because the next step was to find the end of the T-wave by looking at the slope of the signal. Before smoothing, the signal had jagged noise on the T-wave, which caused the algorithm to falsely believe the slope had switched directions. After smoothing, the data was consistent enough to check the difference between each data point. The first time the difference became negative after the S point indicated the location of the top of the T-wave. To ensure that no noise slipped through the smoothing filter and indicated a false peak, the next ten differences were checked to ensure that they were also negative. Then, to find the end of the T-wave, the first time the difference between points turned positive after the peak of the wave indicated the local minimum.
After the Q, S, and T points were found, an average of all QRS-T segments in the data could be determined. This was done by separating out every Q to T interval and averaging them point by point. That average QRS-T segment was then subtracted from each pulse in the signal to, effectively, cancel out the ventricular activity. At this point, the same moving entropy calculation that was discussed along with Equation 3 was completed on this QRS-T removed data.
The third and final method that was completed in this project picked up after the Q, S, and T points were found. The length of each Q to Q interval was determined. Equation 3 was used to calculate the entropy value of those intervals without the use of windowing to split the data up second by second. When all entropy values were determined, they were compared between normal sinus rhythm and atrial fibrillation data. A classification was made of normal sinus rhythm if the Q to Q interval entropy was less than four. If it was greater than four, the signal was classified as AF data.
After the four different entropy calculations were complete, statistical analysis was completed on the values obtained for each signal tested. In total, 20 signals from groups of normal sinus rhythm, atrial fibrillation, and other dysrhythmias, respectively were tested. A student’s T-test was completed on these three sets of data to determine if the average entropy was significantly different between any two pairings. A flow chart of the described methods is shown in Figure 1.
Results
The first thing that was done in this algorithm was to read in the data and normalize it. Data was normalized so that its maximum and minimum values were between zero and one. Then, that data was plotted against time to obtain an idea of what the signal looked like. An example of a normal sinus rhythm is shown in Figure 2 and a rhythm with AF is shown in Figure 3. 
Next, the wavelet decomposition was completed to separate the signal into ten different frequency bands. An example of this breakdown is shown in Figure 4.
The three lowest frequency bands were summed to create a signal that contained only the p-waves of the original ECG. This also removed excess noise on the p-waves. This atrial activity is shown in Figure 5 for a normal ECG and in Figure 6 for an ECG with atrial fibrillation.
Once the atrial activity was separated from the rest of the signal, wavelet entropy was calculated on the atrial activity by looking at each value of the three coefficients that were summed to create the signal. This was done using Equations 1 and 2. The resulting uncertainty of this atrial activity for each signal tested with normal rhythm, AF, and other dysrhythmias is plotted in Figure 7. The wavelet entropy was not significantly different between the three categories of data. It was inconsistently between approximately 0.1 and 0.5 for each classification of signal.
Additionally, normal entropy was calculated by looking at the probabilities of different values within the ECG signal as opposed to the coefficients. It does not appear is if there is a significant difference between the entropy for the atrial activity in any of the three categories. The values are all relatively similar as they were all around 8.1. It is shown, however, that AF signals tend not to drift as low as normal or other dysrhythmic signals. Without further analysis, though,  it does not appear that this method will work for classifying the signal. The calculated entropy values for the three signal types are seen plotted in Figure 8.
Next, the original normalized signal was filtered before time domain analysis was performed. The signal was bandpass filtered with a pass band from 5 to 15 Hz. This eliminated a lot of noise as well as any baseline wander that existed. A filtered normal signal is shown in Figure 7 and a filtered AF signal is shown in Figure 8.
Discussion
The data in Table 1 shows information that goes against the original hypothesis that the entropy of the atrial activity in an ECG exhibiting atrial fibrillation would be higher than that of a normal sinus rhythm. Instead, it shows that entropies of that data, no matter the method used to obtain it, were all relatively equal for the categories of normal sinus rhythm, atrial fibrillation, and other dysrhythmias. 
Figure 4 shows that all of the ventricular data was encapsulated in high frequency bands that were not included in the atrial activity as the shape of the QRS complex can be seen in wavelets with coefficients smaller than d7. However, there is a possibility that some low frequency noise still existed in the end signal. This could add more consistent peaks into the data and disrupt the entropy calculation. It is also possible that some of the atrial activity was left out by not including additional frequencies. This could lead to false results if additional, erratic atrial behavior existed in the original signal, but was not incorporated into the atrial activity.
Figures 14 and 15 show that the detection of the Q, S, and T points was successful as all three points were located correctly very nearly 100 percent of the time. The Q and the S points had 100 percent specificity and sensitivity. The T wave detection was very close behind with 97.4 percent sensitivity and specificity. Therefore, the average QRS complex that was developed was likely to be a good representation of the typical QRS complex in the signal. If the average complex and each individual complex were not perfectly lined up upon cancellation, that could have left additional ventricular activity in the signal, which may have affected entropy calculation. 
An additional possibility for why entropy calculations did not have the expected results is that the atrial activity, alone, was not uncertain enough. When looking closely, the atrial activity in an atrial fibrillation signal is just a number of repeating small peaks. By eliminating the ventricular activity, the inconsistency of how much activity occurred at a time between QRS complexes is removed, so all that is left is a series of small waves. This may have simply not been inconsistent enough on its own to be reflected in entropy values. 
The t-tests that were completed showed strong significance in the difference of means between normal and atrial fibrillation entropies for the Q to Q interval. That, along with the fact that a threshold was able to accurately classify those two categories with 100 percent sensitivity and specificity, shows that Q to Q interval entropy is a good metric to use. The plot in Figure 19 shows a clear divide between all of the entropy values for normal signals and all of the entropy values for AF signals. The t-tests also showed statistical significance in the Q to Q interval between ECGs with atrial fibrillation and other dysrhythmias. While there is statistical significance, there was still a lot of overlap in entropy values between the two, so this may not be a good metric to classify between them as there is not a clear line to divide the categories between. 
Another area that also showed statistical significance was in the entropy of the wavelet determined atrial activity signal in normal versus atrial fibrillation data and atrial fibrillation versus other dysrhythmia data. In both cases, there was still a lot of overlap between the entropy among the pairs, as seen in Figure 20, so at this point that metric is still not useful for classification. However, it does show that there is potential for further research. If the ventricular removal can be strengthened to ensure that absolutely no ventricular activity gets through, but all of the necessary atrial activity still does, that may be a useful tool for classification.
The two additional methods of wavelet entropy on the wavelet determined atrial activity and entropy on the QRS-T cancelled data using an averaged segment were not significant between any of the pairs. As Figures 21 and 22 both display, the entropy values for all three categories of each of these entropy methods are very similar. They are relatively equivalent across the board, so neither of these methods would be of any use for AF classification.
The fact that none of the entropy values showed up as significant in the t-testing between normal signals and other dysrhythmias as well as the fact that the statistically significant methods for atrial fibrillation signals versus other dysrhythmias still showed a lot of overlap in entropy values proves that this algorithm is not robust enough to discern between anything more than normal and atrial fibrillation at this point. Therefore, this could be a tool in diagnosing atrial fibrillation in conjunction with other methods, but not on its own. In some cases, using only this algorithm, a different dysrhythmia could be classified as atrial fibrillation and in others, a signal with some dysrhythmia could be classified as normal. Further analysis would have to be performed on the data, looking at many other features of the ECG in order to improve the ability to pick out atrial fibrillation, specifically.
This algorithm was dependent on the input signals being relatively noise and anomaly free. As long as the data fit that requirement, the software was quite successful. This success is very important because early detection of AF is necessary for the health of individuals who are suffering from it, which is approximately 2 percent of the world population [2]. Accurate detection of dysrhythmias could lead to improved quality of life for millions of people. 




